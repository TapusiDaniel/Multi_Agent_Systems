{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from itertools import count\n",
    "from typing import Union, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size: int = 10000):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        size\n",
    "            Maximum number of transitions store in the buffer.\n",
    "            If the buffer overflows, older states are dropped.\n",
    "        \"\"\"\n",
    "        self.size    = size\n",
    "        self.length  = 0\n",
    "        self.idx     = -1\n",
    "        \n",
    "        # define buffers\n",
    "        self.states        = None\n",
    "        self.states_next   = None\n",
    "        self.actions       = None\n",
    "        self.rewards       = None\n",
    "        self.done          = None\n",
    "        \n",
    "    def store(self, \n",
    "              s: Union[torch.Tensor, np.ndarray], \n",
    "              a: int, \n",
    "              r: float, \n",
    "              s_next: Union[torch.Tensor, np.ndarray],\n",
    "              done: bool):\n",
    "        \n",
    "        \"\"\"\n",
    "        Stores one sample of experience\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s\n",
    "            Tensor encoding the current state.\n",
    "        a\n",
    "            Current action.\n",
    "        r\n",
    "            Current reward.\n",
    "        s_next\n",
    "            Tensor encoding the next state.\n",
    "        done\n",
    "            Done signal.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize buffers\n",
    "        if self.states is None:\n",
    "            self.states      = torch.zeros([self.size] + list(s.shape))   # shape is (self.size, 4)\n",
    "            self.states_next = torch.zeros_like(self.states)              # shape is (self.size, 4)\n",
    "            self.actions     = torch.zeros((self.size, ))                 # shape is (self.size, )\n",
    "            self.rewards     = torch.zeros((self.size, ))                 # shape is (self.size, )\n",
    "            self.done        = torch.zeros((self.size, ))                 # shape is (self.size, ) \n",
    "        \n",
    "        # Convert inputs to tensors if they are numpy arrays\n",
    "        if isinstance(s, np.ndarray):\n",
    "            s = torch.from_numpy(s).float()\n",
    "        if isinstance(s_next, np.ndarray):\n",
    "            s_next = torch.from_numpy(s_next).float()\n",
    "        \n",
    "        # Update the circular buffer by incrementing index\n",
    "        self.idx = (self.idx + 1) % self.size\n",
    "        \n",
    "        # Store the experience directly at the current index\n",
    "        self.states[self.idx] = s.view(-1)  # Flatten the tensor if needed\n",
    "        self.actions[self.idx] = a\n",
    "        self.rewards[self.idx] = r\n",
    "        self.states_next[self.idx] = s_next.view(-1)  # Flatten the tensor if needed\n",
    "        self.done[self.idx] = float(done)\n",
    "        \n",
    "        # Update buffer length\n",
    "        self.length = min(self.length + 1, self.size)\n",
    "            \n",
    "    def sample(self, batch_size: int = 128) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Sample a batch of experience\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size\n",
    "            Number of experience to sample\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of tensor consisting of a batch of states, actions, rewards, next states, done\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.length >= batch_size, \"Can not sample from the buffer yet\"\n",
    "        indices = np.random.choice(a=np.arange(self.length), size=batch_size, replace=False)\n",
    "        \n",
    "        # Sample (s, a, r, s_next, done) behavior samples   \n",
    "        s      = self.states[indices]\n",
    "        s_next = self.states_next[indices]\n",
    "        a = self.actions[indices]\n",
    "        r = self.rewards[indices]\n",
    "        done = self.done[indices]\n",
    "        \n",
    "        return s, a, r, s_next, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network achitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_RAM(nn.Module):\n",
    "    def __init__(self, in_features: int, num_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network for testing algorithm\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features\n",
    "            number of features of input.\n",
    "        num_actions\n",
    "            number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN_RAM, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # define architecture\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_generator(max_eps: float=1.0, min_eps: float=0.1, max_iter: int = 10000):\n",
    "    crt_iter = -1\n",
    "    \n",
    "    while True:\n",
    "        crt_iter += 1\n",
    "        frac = min(crt_iter/max_iter, 1)\n",
    "        eps = (1 - frac) * max_eps + frac * min_eps\n",
    "        yield eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epilson_greedy_action(Q: nn.Module, s: Tensor, eps: float):\n",
    "    rand = np.random.rand()\n",
    "    \n",
    "    # with prob eps select a random action\n",
    "    if rand < eps:\n",
    "        return np.random.choice(np.arange(Q.num_actions))\n",
    "    \n",
    "    # select best action\n",
    "    with torch.no_grad():\n",
    "        output = Q(s).argmax(dim=1).item()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: DQN target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def dqn_target(\n",
    "    Q: nn.Module,\n",
    "    target_Q: nn.Module,\n",
    "    r_batch: Tensor,\n",
    "    s_next_batch: Tensor,\n",
    "    done_batch: Tensor,\n",
    "    gamma: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes DQN target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q\n",
    "        Behavior Q network.\n",
    "    target_Q\n",
    "        Target Q network.\n",
    "    r_batch\n",
    "        Batch of rewards.\n",
    "    s_next_bacth\n",
    "        Batch of next states.\n",
    "    done_batch\n",
    "        Batch of done flag (1 means the episoded finished).\n",
    "    gamma\n",
    "        Discount factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch of DQN targets\n",
    "    \"\"\"\n",
    "    # cmpute next Q value based on which action gives max Q values\n",
    "    # Note:  decorator torch.no_grad() ensures that gradients computed based on next Q are not propagated to the target_Q network\n",
    "    # Note: take note of the done_batch values - if behavior sample i in the batch has a done flag (done_batch[i] = 1), then the next_Q_values[i] \n",
    "    #             will only consider the reward[i] (because there is no next_state)\n",
    "    \n",
    "    next_Q_values = target_Q(s_next_batch).max(dim=1)[0]\n",
    "    next_Q_values[done_batch == 1] = 0\n",
    "    return r_batch + (gamma * next_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: DDQN target\n",
    "\n",
    "$target_t = R_{t+1} + \\gamma Q_{target}(S_{t+1}, \\underset{a}{\\operatorname{argmax}} Q(S_{t+1}, a; \\theta_{t}); \\theta_t^{-})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ddqn_target(\n",
    "    Q: nn.Module,\n",
    "    target_Q: nn.Module,\n",
    "    r_batch: Tensor,\n",
    "    s_next_batch: Tensor,\n",
    "    done_batch: Tensor,\n",
    "    gamma: float) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes DQN target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q\n",
    "        Behavior Q network.\n",
    "    target_Q\n",
    "        Target Q network.\n",
    "    r_batch\n",
    "        Batch of rewards.\n",
    "    s_next_bacth\n",
    "        Batch of next states.\n",
    "    done_batch\n",
    "        Batcho of done flag (1 means the episoded finished).\n",
    "    gamma\n",
    "        Discount factor.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Batch of DQN targets\n",
    "    \"\"\"\n",
    "    # cmpute next Q value based on which action gives max Q values\n",
    "    next_Q_values = target_Q(s_next_batch).gather(1, Q(s_next_batch).argmax(dim=1, keepdim=True)).squeeze()\n",
    "    next_Q_values[done_batch == 1] = 0\n",
    "    \n",
    "    # Compute the target of the current Q values\n",
    "    return r_batch + (gamma * next_Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Learning Alogrithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(\n",
    "    env: gym.Env,\n",
    "    targert_function: Callable,\n",
    "    batch_size: int = 128,\n",
    "    gamma: float = 0.99,\n",
    "    replay_buffer_size=10000,\n",
    "    num_episodes: int = 100000,\n",
    "    learning_starts: int = 1000,\n",
    "    learning_freq: int = 4,\n",
    "    target_update_freq: int = 100,\n",
    "    log_every: int = 100):\n",
    "\n",
    "    \"\"\"\n",
    "    DQN Learning\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        gym environment to train on.\n",
    "    target_function\n",
    "        Function that computes the Q network target. For DQN - dqn_target, for DDQN - ddqn_target.\n",
    "    batch_size:\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma\n",
    "        Discount Factor\n",
    "    replay_buffer_size\n",
    "        Replay buffer size.\n",
    "    num_episodes\n",
    "        number of episodes to run\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    log_every:\n",
    "        Logging interval\n",
    "    \"\"\"\n",
    "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "    input_arg = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # define device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize target q function and q function\n",
    "    Q = DQN_RAM(input_arg, num_actions).to(device)\n",
    "    target_Q = DQN_RAM(input_arg, num_actions).to(device)\n",
    "      \n",
    "    # Construct Q network optimizer function\n",
    "    optimizer = optim.Adam(Q.parameters(), lr=1e-3)\n",
    "    \n",
    "    # define criterion\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    # define epsilon scheduler\n",
    "    eps_scheduler = iter(eps_generator())\n",
    "    \n",
    "    # define statistics buffer, total number of steps and total number of updates performed\n",
    "    all_episode_rewards = []\n",
    "    total_steps = 0\n",
    "    num_param_updates = 0\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # reset environment\n",
    "        s, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for _ in count():\n",
    "            # increse total number of steps\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Choose random action if not yet start learning\n",
    "            if total_steps > learning_starts:\n",
    "                eps = next(eps_scheduler)\n",
    "                s = torch.tensor(s).view(1, -1).float().to(device)\n",
    "                a = select_epilson_greedy_action(Q, s, eps)\n",
    "            else:\n",
    "                a = np.random.choice(np.arange(num_actions))\n",
    "\n",
    "            # advance one step\n",
    "            s_next, r, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # update episode rewards\n",
    "            episode_reward += r\n",
    "\n",
    "            # store other info in replay memory\n",
    "            replay_buffer.store(s, a, r, s_next, done)\n",
    "\n",
    "            # Resets the environment when reaching an episode boundary.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # update state\n",
    "            s = s_next\n",
    "\n",
    "            # perform experience replay and train the network.\n",
    "            if (total_steps > learning_starts and total_steps % learning_freq == 0):\n",
    "                for _ in range(learning_freq):\n",
    "                    if replay_buffer.length >= batch_size:\n",
    "                        # sample experinence from the replay buffer\n",
    "                        s_batch, a_batch, r_batch, s_next_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                        # send everything to device\n",
    "                        s_batch      = s_batch.float().to(device)\n",
    "                        a_batch      = a_batch.long().to(device)\n",
    "                        r_batch      = r_batch.float().to(device)\n",
    "                        s_next_batch = s_next_batch.float().to(device)\n",
    "                        done_batch   = done_batch.long().to(device)\n",
    "\n",
    "                        # comput the q values according to the states and actions\n",
    "                        Q_values = Q(s_batch).gather(1, a_batch.unsqueeze(1)).view(-1)\n",
    "\n",
    "                        # Compute the target of the current Q values\n",
    "                        target_Q_values = targert_function(Q, target_Q, r_batch, s_next_batch, done_batch, gamma)\n",
    "\n",
    "                        # compute loss\n",
    "                        loss = criterion(target_Q_values, Q_values)\n",
    "\n",
    "                        # Clear previous gradients before backward pass\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # increase number of updates\n",
    "                        num_param_updates += 1\n",
    "\n",
    "                        # Periodically update the target network by Q network to target Q network\n",
    "                        if num_param_updates % target_update_freq == 0:\n",
    "                            target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "        # append total reward culumated\n",
    "        all_episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # log average reward over the last 100 episodes\n",
    "        if episode % log_every == 0 and total_steps > learning_starts:\n",
    "            mean_episode_reward = np.mean(all_episode_rewards[-100:])\n",
    "            print(\"Episode: %d, Mean reward: %.2f, Eps: %.2f\" % (episode, mean_episode_reward, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Learning\n",
    "\n",
    "#### Task 4a: Modify learning procedure to implement original DQN (model and target networks are the same)\n",
    "#### Task 4b: Modify learning procedure to implement target network DQN\n",
    "\n",
    "**Note: Experiment with different values of:**\n",
    "  - learning_freq\n",
    "  - target_update_frequency\n",
    "  - epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danyez87/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Mean reward: 22.28, Eps: 0.89\n",
      "Episode: 200, Mean reward: 36.00, Eps: 0.57\n",
      "Episode: 300, Mean reward: 105.97, Eps: 0.10\n",
      "Episode: 400, Mean reward: 326.21, Eps: 0.10\n",
      "Episode: 500, Mean reward: 397.84, Eps: 0.10\n",
      "Episode: 600, Mean reward: 390.41, Eps: 0.10\n",
      "Episode: 700, Mean reward: 248.07, Eps: 0.10\n",
      "Episode: 800, Mean reward: 323.72, Eps: 0.10\n",
      "Episode: 900, Mean reward: 247.35, Eps: 0.10\n",
      "Episode: 1000, Mean reward: 312.33, Eps: 0.10\n"
     ]
    }
   ],
   "source": [
    "# initialize gym env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# DQN learning\n",
    "learning(\n",
    "    env=env,                          # gym environmnet\n",
    "    targert_function=dqn_target,      # dqn target construction\n",
    "    batch_size=128,                   # q-network update batch size\n",
    "    gamma=0.99,                       # discount factor\n",
    "    replay_buffer_size=10000,         # size of the replay buffer\n",
    "    num_episodes=1000,                # number of episodes to run\n",
    "    learning_starts=1000,             # number of initial random actions (exploration)\n",
    "    learning_freq=4,                  # frequency of the update\n",
    "    target_update_freq=100,           # number of gradient steps after which the target network is updated\n",
    "    log_every=100                     # logging interval. returns the mean reward per episode.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Learning\n",
    "**Note: Experiment with different values of:**\n",
    "  - learning_freq\n",
    "  - target_update_frequency\n",
    "  - epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Mean reward: 22.78, Eps: 0.89\n",
      "Episode: 200, Mean reward: 38.83, Eps: 0.54\n",
      "Episode: 300, Mean reward: 142.05, Eps: 0.10\n",
      "Episode: 400, Mean reward: 174.32, Eps: 0.10\n",
      "Episode: 500, Mean reward: 161.36, Eps: 0.10\n",
      "Episode: 600, Mean reward: 156.22, Eps: 0.10\n",
      "Episode: 700, Mean reward: 155.82, Eps: 0.10\n",
      "Episode: 800, Mean reward: 157.58, Eps: 0.10\n",
      "Episode: 900, Mean reward: 144.34, Eps: 0.10\n",
      "Episode: 1000, Mean reward: 154.17, Eps: 0.10\n"
     ]
    }
   ],
   "source": [
    "# initialize gym env\n",
    "env = gym.make(\"CartPole-v1\", max_episode_steps=200)\n",
    "\n",
    "# DQN learning\n",
    "learning(\n",
    "    env=env,                          # gym environmnet\n",
    "    targert_function=ddqn_target,     # dqn target construction\n",
    "    batch_size=128,                   # q-network update batch size\n",
    "    gamma=0.99,                       # discount factor\n",
    "    replay_buffer_size=10000,         # size of the replay buffer\n",
    "    num_episodes=1000,                # number of episodes to run\n",
    "    learning_starts=1000,             # number of initial random actions  (exploration)\n",
    "    learning_freq=4,                  # frequency of the update\n",
    "    target_update_freq=100,           # number of gradient steps after which the target network is updated\n",
    "    log_every=100                     # logging interval. returns the mean reward per episode.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_best_results(dqn_results, ddqn_results):\n",
    "    \"\"\"\n",
    "    Compare the best performing configurations of DQN and Double DQN\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dqn_results: dict\n",
    "        Results from DQN experiments\n",
    "    ddqn_results: dict\n",
    "        Results from Double DQN experiments\n",
    "    \"\"\"\n",
    "    # Function to get the best configuration based on last 100 episode average\n",
    "    def get_best_config(results):\n",
    "        best_key = max(results.keys(), key=lambda k: np.mean(results[k][-100:]))\n",
    "        return best_key, results[best_key]\n",
    "    \n",
    "    # Get best DQN and DDQN results\n",
    "    best_dqn_key, best_dqn_rewards = get_best_config(dqn_results)\n",
    "    best_ddqn_key, best_ddqn_rewards = get_best_config(ddqn_results)\n",
    "    \n",
    "    # Calculate smoothed rewards\n",
    "    window_size = 100\n",
    "    dqn_smoothed = [np.mean(best_dqn_rewards[max(0, i-window_size):i+1]) for i in range(len(best_dqn_rewards))]\n",
    "    ddqn_smoothed = [np.mean(best_ddqn_rewards[max(0, i-window_size):i+1]) for i in range(len(best_ddqn_rewards))]\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot DQN results\n",
    "    plt.plot(dqn_smoothed, label=f'Best DQN: {best_dqn_key}', color='blue', linewidth=2)\n",
    "    \n",
    "    # Plot DDQN results\n",
    "    plt.plot(ddqn_smoothed, label=f'Best DDQN: {best_ddqn_key}', color='green', linewidth=2)\n",
    "    \n",
    "    # Add reference line for \"solved\" threshold\n",
    "    plt.axhline(y=195, color='r', linestyle='--', label='Solved threshold (195)')\n",
    "    \n",
    "    plt.title('Comparison of Best DQN vs Double DQN Configurations')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward (100-episode window)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('dqn_vs_ddqn_comparison.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nBest DQN Configuration: {best_dqn_key}\")\n",
    "    print(f\"Final Average Reward: {np.mean(best_dqn_rewards[-100:]):.2f}\")\n",
    "    \n",
    "    print(f\"\\nBest DDQN Configuration: {best_ddqn_key}\")\n",
    "    print(f\"Final Average Reward: {np.mean(best_ddqn_rewards[-100:]):.2f}\")\n",
    "    \n",
    "    return best_dqn_key, best_ddqn_key\n",
    "\n",
    "# Compare the best results\n",
    "best_dqn_config, best_ddqn_config = compare_best_results(dqn_results, ddqn_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_effects(results, parameter_name):\n",
    "    \"\"\"\n",
    "    Analyze how a specific parameter affects performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results: dict\n",
    "        Results from experiments\n",
    "    parameter_name: str\n",
    "        The parameter to analyze: 'lf', 'tuf', or 'eps'\n",
    "    \"\"\"\n",
    "    # Extract values for the specified parameter\n",
    "    if parameter_name == 'lf':\n",
    "        param_values = [1, 4, 8]\n",
    "        param_full_name = 'Learning Frequency'\n",
    "        param_extract = lambda k: int(k.split('_lf')[1].split('_')[0])\n",
    "    elif parameter_name == 'tuf':\n",
    "        param_values = [10, 100, 200]\n",
    "        param_full_name = 'Target Update Frequency'\n",
    "        param_extract = lambda k: int(k.split('_tuf')[1].split('_')[0])\n",
    "    elif parameter_name == 'eps':\n",
    "        param_values = [5000, 10000, 20000]\n",
    "        param_full_name = 'Epsilon Decay Steps'\n",
    "        param_extract = lambda k: int(k.split('_eps')[1] if '_eps' in k else k.split('eps')[1])\n",
    "    \n",
    "    # Group results by parameter value\n",
    "    grouped_results = {value: [] for value in param_values}\n",
    "    \n",
    "    for key, rewards in results.items():\n",
    "        try:\n",
    "            param_value = param_extract(key)\n",
    "            if param_value in param_values:\n",
    "                grouped_results[param_value].append(rewards)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate average performance for each parameter value\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    \n",
    "    for value, reward_lists in grouped_results.items():\n",
    "        if reward_lists:\n",
    "            # Calculate the average reward across all configurations with this parameter value\n",
    "            avg_rewards = np.mean([np.mean(rewards[-100:]) for rewards in reward_lists])\n",
    "            x_values.append(value)\n",
    "            y_values.append(avg_rewards)\n",
    "    \n",
    "    # Sort by parameter value for proper display\n",
    "    sorted_data = sorted(zip(x_values, y_values))\n",
    "    x_values = [x for x, _ in sorted_data]\n",
    "    y_values = [y for _, y in sorted_data]\n",
    "    \n",
    "    plt.bar([str(x) for x in x_values], y_values)\n",
    "    plt.title(f'Effect of {param_full_name} on Performance')\n",
    "    plt.xlabel(param_full_name)\n",
    "    plt.ylabel('Average Reward (last 100 episodes)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'effect_of_{parameter_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the effect of each parameter for DQN\n",
    "print(\"\\nAnalyzing parameter effects for DQN:\")\n",
    "analyze_parameter_effects(dqn_results, 'lf')\n",
    "analyze_parameter_effects(dqn_results, 'tuf')\n",
    "analyze_parameter_effects(dqn_results, 'eps')\n",
    "\n",
    "# Analyze the effect of each parameter for Double DQN\n",
    "print(\"\\nAnalyzing parameter effects for Double DQN:\")\n",
    "analyze_parameter_effects(ddqn_results, 'lf')\n",
    "analyze_parameter_effects(ddqn_results, 'tuf')\n",
    "analyze_parameter_effects(ddqn_results, 'eps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
